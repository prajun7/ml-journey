{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa4e0258-6205-4cb6-bcf3-29660cba8daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "PyTorch Version: 2.6.0\n",
      "Torchvision Version: 0.21.0\n"
     ]
    }
   ],
   "source": [
    "# We start by importing all the required modules from PyTorch and other libraries.\n",
    "# - `torch`: The main PyTorch library.\n",
    "# - `torch.nn`: Contains building blocks for neural networks (layers, loss functions).\n",
    "# - `torch.optim`: Provides optimization algorithms (like Adam, SGD).\n",
    "# - `torchvision`: Offers popular datasets, model architectures, and image transformations.\n",
    "# - `torchvision.transforms`: Contains common image transformations.\n",
    "# - `torch.utils.data.DataLoader`: Handles batching and shuffling of data.\n",
    "# - `tqdm`: A utility for creating smart progress bars for loops.\n",
    "# - `torch.nn.functional as F`: Provides functional interfaces for some nn operations (though we mostly use nn.Module layers here).\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch.nn.functional as F # Often used for activation functions if not defined as layers\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0260e4a6-9e13-40a5-bbc8-1f381b6c7fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Batch Size: 64\n",
      "  Learning Rate: 0.001\n",
      "  Number of Epochs: 20\n",
      "  Number of Classes: 100\n",
      "  Data Directory: ./data_cifar100\n",
      "  Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Hyperparameters\n",
    "# Define key parameters for the dataset, model, and training process.\n",
    "# - `BATCH_SIZE`: How many images are processed in one go. Affects memory usage and training dynamics.\n",
    "# - `LEARNING_RATE`: Controls how much the model weights are adjusted during optimization.\n",
    "# - `NUM_EPOCHS`: How many times the entire training dataset is passed through the model.\n",
    "# - `NUM_CLASSES`: CIFAR-100 has 100 distinct image categories.\n",
    "# - `DATA_DIR`: Where to download and store the CIFAR-100 dataset locally.\n",
    "# - `DEVICE`: Automatically select GPU ('cuda') if available, otherwise use CPU ('cpu'). Training is much faster on GPU.\n",
    "\n",
    "# %%\n",
    "BATCH_SIZE = 64          # Number of images per batch\n",
    "LEARNING_RATE = 0.001    # Learning rate for the optimizer\n",
    "NUM_EPOCHS = 20          # Number of times to iterate over the entire dataset\n",
    "NUM_CLASSES = 100        # CIFAR-100 has 100 classes\n",
    "DATA_DIR = './data_cifar100' # Directory to store dataset\n",
    "\n",
    "# Determine the device to run the training on (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"  Data Directory: {DATA_DIR}\")\n",
    "print(f\"  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995cadc9-df92-4e06-ba64-6b5570cc9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformations defined:\n",
      "Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 3: Define Data Transformations\n",
    "# Specify the preprocessing steps to apply to each image.\n",
    "# - `transforms.ToTensor()`: Converts PIL Images (or NumPy arrays) with pixel values in [0, 255] range to PyTorch Tensors with values in [0.0, 1.0]. It also rearranges dimensions from HxWxC to CxHxW (Channels first).\n",
    "# - `transforms.Normalize()`: Normalizes tensor image channels using specified mean and standard deviation. This helps stabilize training by centering data around zero. We use (0.5, 0.5, 0.5) for both mean and std to scale pixel values from [0, 1] to [-1, 1]. Other common choices are pre-calculated means/stds specific to CIFAR-100.\n",
    "# - `transforms.Compose()`: Chains multiple transformations together.\n",
    "\n",
    "# %%\n",
    "# Define the sequence of transformations for the input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert image to PyTorch Tensor (scales to [0, 1])\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # Normalize to [-1, 1] range\n",
    "    # Alternative: Use pre-calculated CIFAR-100 means/stds\n",
    "    # transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "print(\"Data transformations defined:\")\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8be26c35-dd42-4f9c-9b1f-15bfe9f6b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset loaded. Number of samples: 50000\n",
      "Number of classes: 100\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 4: Load Training Dataset\n",
    "# Download (if needed) and load the CIFAR-100 training set using `torchvision.datasets`.\n",
    "# - `root`: Specifies the directory to store the data.\n",
    "# - `train=True`: Indicates that we want the training split of the dataset.\n",
    "# - `download=True`: Allows downloading the dataset if it's not found in the `root` directory.\n",
    "# - `transform`: Applies the predefined transformations to each image.\n",
    "\n",
    "# %%\n",
    "print(\"Loading Training Dataset...\")\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=DATA_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "print(f\"Training dataset loaded. Number of samples: {len(train_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "# print(f\"Class names sample: {train_dataset.classes[:10]}\") # Uncomment to see some class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "607d6251-b084-43c4-8141-05de35dfce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders...\n",
      "DataLoaders created.\n",
      "  Number of training batches: 782\n",
      "  Number of testing batches: 157\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 6: Create DataLoaders\n",
    "# Wrap the datasets in `DataLoader` objects. This handles:\n",
    "# - **Batching:** Grouping data samples into batches of size `BATCH_SIZE`.\n",
    "# - **Shuffling:** Randomly shuffling the training data at the beginning of each epoch to improve generalization (`shuffle=True` for train_loader). Shuffling is typically not needed for the test set.\n",
    "# - **Parallel Loading:** Using multiple worker processes (`num_workers`) to load data in the background, speeding up training.\n",
    "\n",
    "# %%\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=2  # Adjust based on your system's capability\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False, # No need to shuffle test data\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created.\")\n",
    "print(f\"  Number of training batches: {len(train_loader)}\")\n",
    "print(f\"  Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "844f3e2e-507d-492d-a4eb-2ddc850a33c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting a sample batch...\n",
      "  Images batch shape: torch.Size([64, 3, 32, 32])\n",
      "  Labels batch shape: torch.Size([64])\n",
      "  Example label value: 69\n",
      "  Image tensor min value: -1.00\n",
      "  Image tensor max value: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Inspect a Batch \n",
    "# It's good practice to check the output of the DataLoader. We fetch one batch and print the shape of the images and labels tensors to ensure they match expectations.\n",
    "\n",
    "# %%\n",
    "print(\"Inspecting a sample batch...\")\n",
    "# Get one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(f\"  Images batch shape: {images.shape}\")\n",
    "# Expected: [BATCH_SIZE, 3 (Channels), 32 (Height), 32 (Width)]\n",
    "\n",
    "print(f\"  Labels batch shape: {labels.shape}\")\n",
    "# Expected: [BATCH_SIZE]\n",
    "\n",
    "print(f\"  Example label value: {labels[0].item()}\") # Print the label of the first image in the batch\n",
    "print(f\"  Image tensor min value: {images.min():.2f}\") # Check normalization effect\n",
    "print(f\"  Image tensor max value: {images.max():.2f}\") # Check normalization effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26a840ab-9221-4ebd-a70e-8014cc021b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model Class Defined.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 8: Define the CNN Model Architecture\n",
    "# Define the structure of our Convolutional Neural Network using `nn.Module`.\n",
    "# The architecture follows the proposal: Conv -> ReLU -> Pool -> Conv -> ReLU -> Pool -> Conv -> ReLU -> Pool -> Flatten -> FC -> ReLU -> Dropout -> FC.\n",
    "# - `nn.Conv2d`: Applies 2D convolution. `padding=1` with `kernel_size=3` preserves spatial dimensions before pooling.\n",
    "# - `nn.ReLU`: Rectified Linear Unit activation function introduces non-linearity.\n",
    "# - `nn.MaxPool2d`: Reduces spatial dimensions (height, width) by taking the max value in a window, helping to make the model more robust to variations in feature positions and reducing computation. `kernel_size=2, stride=2` halves the dimensions.\n",
    "# - `nn.Flatten`: Converts the 3D feature map (Channels x Height x Width) from the conv layers into a 1D vector suitable for Fully Connected layers.\n",
    "# - `nn.Linear`: Applies a linear transformation (fully connected layer). The input size `128 * 4 * 4` is calculated based on the output shape after the last pooling layer (128 channels, 4x4 spatial size).\n",
    "# - `nn.Dropout`: Randomly sets a fraction (`p=0.5`) of input units to 0 during training, acting as a regularization technique to prevent overfitting.\n",
    "# - **Note on Softmax:** We don't add a `nn.Softmax` layer at the end because `nn.CrossEntropyLoss` (used later) internally computes Softmax for numerical stability and efficiency. The model outputs raw scores (logits).\n",
    "\n",
    "# %%\n",
    "class CIFAR100_CNN(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network architecture for CIFAR-100.\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CIFAR100_CNN, self).__init__()\n",
    "\n",
    "        # --- Convolutional Block 1 ---\n",
    "        # Input: [BATCH_SIZE, 3, 32, 32]\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # Output: [BATCH_SIZE, 32, 32, 32] (padding preserves size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output: [BATCH_SIZE, 32, 16, 16] (pooling halves size)\n",
    "\n",
    "        # --- Convolutional Block 2 ---\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # Output: [BATCH_SIZE, 64, 16, 16]\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output: [BATCH_SIZE, 64, 8, 8]\n",
    "\n",
    "        # --- Convolutional Block 3 ---\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        # Output: [BATCH_SIZE, 128, 8, 8]\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output: [BATCH_SIZE, 128, 4, 4]\n",
    "\n",
    "        # --- Flatten Layer ---\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Output: [BATCH_SIZE, 128 * 4 * 4] = [BATCH_SIZE, 2048]\n",
    "\n",
    "        # --- Fully Connected Block ---\n",
    "        # Calculate flattened features size: channels * height * width = 128 * 4 * 4 = 2048\n",
    "        self.fc1 = nn.Linear(in_features=128 * 4 * 4, out_features=512)\n",
    "        # Output: [BATCH_SIZE, 512]\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5) # Dropout layer for regularization\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        # Output: [BATCH_SIZE, NUM_CLASSES] (raw logits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the network.\"\"\"\n",
    "        # Pass through convolutional blocks\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x) # Apply dropout during training\n",
    "        x = self.fc2(x)     # Final output logits\n",
    "        return x\n",
    "\n",
    "print(\"CNN Model Class Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7becf63-b2d8-4765-932d-591e424aac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated and moved to device:\n",
      "CIFAR100_CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 9: Instantiate Model and Move to Device\n",
    "# Create an instance of our defined `CIFAR100_CNN` model.\n",
    "# Then, move the model's parameters and buffers to the selected `device` (GPU or CPU). This ensures computations happen on the desired hardware.\n",
    "\n",
    "# %%\n",
    "model = CIFAR100_CNN(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Print the model structure (optional, but useful)\n",
    "print(\"Model instantiated and moved to device:\")\n",
    "print(model)\n",
    "\n",
    "# You can also count parameters (optional)\n",
    "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"Total trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2983eeb4-1f1d-47fe-ac7b-2f3de87ed8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function defined: CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 10: Define Loss Function\n",
    "# Choose the loss function (or criterion) to measure the difference between the model's predictions and the actual labels.\n",
    "# - `nn.CrossEntropyLoss`: This is the standard loss function for multi-class classification problems. It combines `nn.LogSoftmax` and `nn.NLLLoss` (Negative Log Likelihood Loss) in one class, making it numerically stable and convenient. It expects raw logits from the model (output of the last linear layer) and integer class labels.\n",
    "\n",
    "# %%\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"Loss function defined: {criterion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f9bf32e-8b8f-4c95-916e-1f4bae7c6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer defined: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 11: Define Optimizer\n",
    "# Select the optimization algorithm that will update the model's weights based on the gradients calculated during backpropagation.\n",
    "# - `optim.Adam`: A popular and generally effective adaptive learning rate optimization algorithm. We pass it the model's parameters (`model.parameters()`) which need to be updated, and the `LEARNING_RATE`.\n",
    "\n",
    "# %%\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"Optimizer defined: {optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "478caf08-ef58-4fdc-b9d3-d8881a25b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 20 epochs on cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.98it/s, Loss=3.6262, Acc\n",
      "Epoch 1/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.90it/s, Loss=2.9946, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 Completed:\n",
      "  Train Loss: 3.8291, Train Accuracy: 10.87%\n",
      "  Validation Loss: 3.2081, Validation Accuracy: 22.04%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.81it/s, Loss=3.1060, Acc\n",
      "Epoch 2/20 [Validation]: 100%|█| 157/157 [00:20<00:00,  7.60it/s, Loss=2.8664, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20 Completed:\n",
      "  Train Loss: 3.1451, Train Accuracy: 22.59%\n",
      "  Validation Loss: 2.7949, Validation Accuracy: 30.69%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Training]: 100%|█| 782/782 [00:54<00:00, 14.46it/s, Loss=2.2586, Acc\n",
      "Epoch 3/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.91it/s, Loss=2.6217, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20 Completed:\n",
      "  Train Loss: 2.8235, Train Accuracy: 28.75%\n",
      "  Validation Loss: 2.5553, Validation Accuracy: 35.43%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Training]: 100%|█| 782/782 [00:53<00:00, 14.58it/s, Loss=2.4450, Acc\n",
      "Epoch 4/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.85it/s, Loss=2.3826, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20 Completed:\n",
      "  Train Loss: 2.6153, Train Accuracy: 32.86%\n",
      "  Validation Loss: 2.4498, Validation Accuracy: 37.52%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Training]: 100%|█| 782/782 [00:57<00:00, 13.70it/s, Loss=2.1776, Acc\n",
      "Epoch 5/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.87it/s, Loss=2.3859, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20 Completed:\n",
      "  Train Loss: 2.4482, Train Accuracy: 36.05%\n",
      "  Validation Loss: 2.3352, Validation Accuracy: 39.49%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.91it/s, Loss=2.7451, Acc\n",
      "Epoch 6/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.86it/s, Loss=2.0883, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20 Completed:\n",
      "  Train Loss: 2.3075, Train Accuracy: 39.13%\n",
      "  Validation Loss: 2.2768, Validation Accuracy: 40.66%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.95it/s, Loss=1.5589, Acc\n",
      "Epoch 7/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.89it/s, Loss=2.1416, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20 Completed:\n",
      "  Train Loss: 2.1865, Train Accuracy: 41.89%\n",
      "  Validation Loss: 2.1924, Validation Accuracy: 42.35%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Training]: 100%|█| 782/782 [00:51<00:00, 15.07it/s, Loss=1.5151, Acc\n",
      "Epoch 8/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.90it/s, Loss=2.2093, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20 Completed:\n",
      "  Train Loss: 2.0950, Train Accuracy: 43.94%\n",
      "  Validation Loss: 2.1718, Validation Accuracy: 43.61%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Training]: 100%|█| 782/782 [00:52<00:00, 15.00it/s, Loss=2.2427, Acc\n",
      "Epoch 9/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.84it/s, Loss=2.1499, A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20 Completed:\n",
      "  Train Loss: 2.0093, Train Accuracy: 45.37%\n",
      "  Validation Loss: 2.1659, Validation Accuracy: 43.25%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.95it/s, Loss=1.2119, Ac\n",
      "Epoch 10/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.90it/s, Loss=2.1258, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20 Completed:\n",
      "  Train Loss: 1.9165, Train Accuracy: 47.64%\n",
      "  Validation Loss: 2.1480, Validation Accuracy: 44.12%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Training]: 100%|█| 782/782 [00:52<00:00, 15.02it/s, Loss=1.6145, Ac\n",
      "Epoch 11/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.83it/s, Loss=2.2520, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20 Completed:\n",
      "  Train Loss: 1.8379, Train Accuracy: 49.10%\n",
      "  Validation Loss: 2.1662, Validation Accuracy: 43.71%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.97it/s, Loss=1.9742, Ac\n",
      "Epoch 12/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.89it/s, Loss=2.4174, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20 Completed:\n",
      "  Train Loss: 1.7674, Train Accuracy: 50.88%\n",
      "  Validation Loss: 2.1496, Validation Accuracy: 44.60%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.99it/s, Loss=1.3338, Ac\n",
      "Epoch 13/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.90it/s, Loss=2.1391, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20 Completed:\n",
      "  Train Loss: 1.7015, Train Accuracy: 52.17%\n",
      "  Validation Loss: 2.1619, Validation Accuracy: 44.26%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.96it/s, Loss=1.4059, Ac\n",
      "Epoch 14/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.87it/s, Loss=1.9508, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20 Completed:\n",
      "  Train Loss: 1.6448, Train Accuracy: 53.34%\n",
      "  Validation Loss: 2.1415, Validation Accuracy: 44.77%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Training]: 100%|█| 782/782 [19:48<00:00,  1.52s/it, Loss=1.8608, Ac\n",
      "Epoch 15/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.77it/s, Loss=1.8772, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20 Completed:\n",
      "  Train Loss: 1.5756, Train Accuracy: 55.02%\n",
      "  Validation Loss: 2.2057, Validation Accuracy: 44.50%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.82it/s, Loss=1.5751, Ac\n",
      "Epoch 16/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.84it/s, Loss=2.0070, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20 Completed:\n",
      "  Train Loss: 1.5159, Train Accuracy: 56.17%\n",
      "  Validation Loss: 2.1978, Validation Accuracy: 45.03%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Training]: 100%|█| 782/782 [00:52<00:00, 15.01it/s, Loss=1.3355, Ac\n",
      "Epoch 17/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.93it/s, Loss=1.8481, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20 Completed:\n",
      "  Train Loss: 1.4631, Train Accuracy: 57.62%\n",
      "  Validation Loss: 2.2239, Validation Accuracy: 44.81%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.97it/s, Loss=1.4990, Ac\n",
      "Epoch 18/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.97it/s, Loss=2.2168, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20 Completed:\n",
      "  Train Loss: 1.4256, Train Accuracy: 58.47%\n",
      "  Validation Loss: 2.2720, Validation Accuracy: 44.23%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.96it/s, Loss=1.4812, Ac\n",
      "Epoch 19/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  8.89it/s, Loss=2.1529, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20 Completed:\n",
      "  Train Loss: 1.3757, Train Accuracy: 59.45%\n",
      "  Validation Loss: 2.2592, Validation Accuracy: 45.04%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Training]: 100%|█| 782/782 [00:52<00:00, 14.88it/s, Loss=1.0136, Ac\n",
      "Epoch 20/20 [Validation]: 100%|█| 157/157 [00:17<00:00,  9.01it/s, Loss=1.7050, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20 Completed:\n",
      "  Train Loss: 1.3217, Train Accuracy: 60.84%\n",
      "  Validation Loss: 2.3225, Validation Accuracy: 44.09%\n",
      "--------------------------------------------------\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 12: Training and Validation Loop\n",
    "# This is the core part where the model learns. We iterate for `NUM_EPOCHS`.\n",
    "# In each epoch:\n",
    "# 1.  **Training Phase:**\n",
    "#     - Set the model to training mode (`model.train()`). This enables layers like Dropout.\n",
    "#     - Iterate through batches from the `train_loader`.\n",
    "#     - Move data to the `device`.\n",
    "#     - **Zero Gradients:** Clear gradients from the previous batch (`optimizer.zero_grad()`).\n",
    "#     - **Forward Pass:** Get model predictions (`outputs = model(inputs)`).\n",
    "#     - **Calculate Loss:** Compute the loss using `criterion`.\n",
    "#     - **Backward Pass:** Calculate gradients (`loss.backward()`).\n",
    "#     - **Optimize:** Update model weights (`optimizer.step()`).\n",
    "#     - Track loss and accuracy.\n",
    "# 2.  **Validation Phase:**\n",
    "#     - Set the model to evaluation mode (`model.eval()`). This disables layers like Dropout and adjusts layers like BatchNorm.\n",
    "#     - Disable gradient computation (`with torch.no_grad():`) for efficiency, as we don't need gradients during evaluation.\n",
    "#     - Iterate through batches from the `test_loader`.\n",
    "#     - Calculate loss and accuracy on the test set.\n",
    "# 3.  Print summary statistics for the epoch.\n",
    "\n",
    "# %%\n",
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs on {device}...\")\n",
    "\n",
    "# Loop over the dataset multiple times\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train()  # Set model to training mode (enables dropout, etc.)\n",
    "    running_loss_train = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Use tqdm for a progress bar over the training batches\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\")\n",
    "\n",
    "    for inputs, labels in train_pbar:\n",
    "        # Move inputs and labels to the configured device (GPU/CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # --- Core Training Steps ---\n",
    "        # 1. Zero the parameter gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 3. Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 4. Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # --- End Core Training Steps ---\n",
    "\n",
    "        # Update statistics for the current batch\n",
    "        running_loss_train += loss.item() * inputs.size(0) # loss.item() is avg loss per item in batch\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the index of the max log-probability\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar description dynamically\n",
    "        current_acc_train = 100. * correct_train / total_train\n",
    "        train_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{current_acc_train:.2f}%\"})\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch (training)\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    epoch_acc_train = 100. * correct_train / total_train\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "    running_loss_val = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    # Disable gradient calculation during validation for efficiency\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Validation]\")\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update statistics\n",
    "            running_loss_val += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            current_acc_val = 100. * correct_val / total_val\n",
    "            val_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{current_acc_val:.2f}%\"})\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch (validation)\n",
    "    epoch_loss_val = running_loss_val / len(test_loader.dataset)\n",
    "    epoch_acc_val = 100. * correct_val / total_val\n",
    "\n",
    "    # Print summary for the epoch\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} Completed:\")\n",
    "    print(f\"  Train Loss: {epoch_loss_train:.4f}, Train Accuracy: {epoch_acc_train:.2f}%\")\n",
    "    print(f\"  Validation Loss: {epoch_loss_val:.4f}, Validation Accuracy: {epoch_acc_val:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c70ba20-d51d-42e7-9063-af41a1dac64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Cell 13: Save the Trained Model (Optional)\n",
    "# After training, you might want to save the model's learned parameters (weights and biases) for later use (e.g., inference or further training).\n",
    "# We typically save the `state_dict`, which is a Python dictionary object that maps each layer to its parameter tensor.\n",
    "\n",
    "# %%\n",
    "# Define path to save the model\n",
    "model_save_path = \"cifar100_cnn_model.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# print(f\"Model state dictionary saved to {model_save_path}\")\n",
    "\n",
    "# To load the model later:\n",
    "# model_loaded = CIFAR100_CNN(num_classes=NUM_CLASSES) # Create instance first\n",
    "# model_loaded.load_state_dict(torch.load(model_save_path))\n",
    "# model_loaded.to(device) # Move to device\n",
    "# model_loaded.eval() # Set to evaluation mode if using for inference\n",
    "# print(\"Model loaded successfully (Example).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4200550-33fe-4948-aab3-dfdf7ba7a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_ml_research]",
   "language": "python",
   "name": "conda-env-env_ml_research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
