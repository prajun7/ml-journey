{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "40ea3f9f-5eda-4558-abbf-d6f6ce85d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "TensorFlow Version: 2.16.2\n",
      "Keras Version: 3.9.0\n",
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU memory growth configured.\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow, Keras components, and other utilities.\n",
    "# - tensorflow as tf: The core TensorFlow library.\n",
    "# - tensorflow.keras : TensorFlow's high-level API for building and training models.\n",
    "# - layers: Module containing standard neural network layers (Conv2D, Dense, etc.).\n",
    "# - models: Module for creating models (Sequential, Functional API).\n",
    "# - datasets: Module containing built-in datasets like CIFAR-100.\n",
    "# - optimizers: Module containing optimization algorithms (Adam, SGD, etc.).\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, datasets, optimizers, losses\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Suppress TensorFlow informational messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "\n",
    "# Check for GPU availability\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"GPU available: {gpu_devices}\")\n",
    "    # Optional: Configure GPU memory growth to avoid allocating all memory at once\n",
    "    try:\n",
    "        for gpu in gpu_devices:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth configured.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"GPU not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b2b75992-bdfe-4ec1-afee-63afe43337e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Batch Size: 64\n",
      "  Learning Rate: 0.001\n",
      "  Number of Epochs: 100\n",
      "  Number of Classes: 100\n",
      "  Input Shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Hyperparameters\n",
    "# - BATCH_SIZE: Number of images processed in one training step.\n",
    "# - LEARNING_RATE: Controls the step size during optimization.\n",
    "# - NUM_EPOCHS: How many times the entire training dataset is passed through the model.\n",
    "# - NUM_CLASSES: CIFAR-100 has 100 distinct image categories.\n",
    "# - INPUT_SHAPE: The dimensions of each input image (Height, Width, Channels).\n",
    "# - L2_LAMBDA\n",
    "\n",
    "BATCH_SIZE = 64          # Number of images per batch\n",
    "LEARNING_RATE = 0.001    # Learning rate for the optimizer\n",
    "NUM_EPOCHS = 100         # Number of times to iterate over the entire dataset\n",
    "NUM_CLASSES = 100        # CIFAR-100 has 100 classes\n",
    "INPUT_SHAPE = (32, 32, 3) # CIFAR images are 32x32 pixels with 3 color channels (RGB)\n",
    "L2_LAMBDA = 1e-3 # Define L2 regularization strength\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Number of Classes: {NUM_CLASSES}\")\n",
    "print(f\"  Input Shape: {INPUT_SHAPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3c6deb9c-595f-4005-9acc-e05e702f7701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-100 dataset...\n",
      "Dataset loaded successfully.\n",
      "  x_train shape: (50000, 32, 32, 3)\n",
      "  y_train shape: (50000, 1)\n",
      "  x_test shape: (10000, 32, 32, 3)\n",
      "  y_test shape: (10000, 1)\n",
      "  Number of training samples: 50000\n",
      "  Number of test samples: 10000\n",
      "  Image data type: uint8\n",
      "  Label data type: int64\n",
      "  Min/Max pixel values: 0/255\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset directly using `tf.keras.datasets.cifar100`.\n",
    "# This function returns NumPy arrays for training and testing images and labels.\n",
    "# - Images (`x_train`, `x_test`) are NumPy arrays of shape (num_samples, 32, 32, 3) with pixel values in [0, 255].\n",
    "# - Labels (`y_train`, `y_test`) are NumPy arrays of shape (num_samples, 1) containing integer labels from 0 to 99.\n",
    "\n",
    "print(\"Loading CIFAR-100 dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(f\"  x_train shape: {x_train.shape}\") # (50000, 32, 32, 3)\n",
    "# x_train holds 50,000 images. Each of those 50,000 images is a 32x32 grid of pixels.  \n",
    "# And at each of those 32x32 pixel locations, there are 3 values representing the \n",
    "# Red, Green, and Blue color components of that pixel.\n",
    "\n",
    "print(f\"  y_train shape: {y_train.shape}\") # (50000, 1)\n",
    "print(f\"  x_test shape: {x_test.shape}\")   # (10000, 32, 32, 3)\n",
    "print(f\"  y_test shape: {y_test.shape}\")   # (10000, 1)\n",
    "print(f\"  Number of training samples: {x_train.shape[0]}\")\n",
    "print(f\"  Number of test samples: {x_test.shape[0]}\")\n",
    "print(f\"  Image data type: {x_train.dtype}\") # uint8\n",
    "print(f\"  Label data type: {y_train.dtype}\") # int64\n",
    "print(f\"  Min/Max pixel values: {x_train.min()}/{x_train.max()}\") # 0/255\n",
    "\n",
    "# # Print the first 2 rows of x_train (image data)\n",
    "# print(\"\\nFirst 2 rows of x_train (image data):\")\n",
    "# print(x_train[:2])  # Print the first 2 image arrays.\n",
    "\n",
    "# # Print the first 2 rows of y_train (labels)\n",
    "# print(\"\\nFirst 2 rows of y_train (labels):\")\n",
    "# print(y_train[:2])\n",
    "\n",
    "# # Print the first 2 rows of x_test (image data)\n",
    "# print(\"\\nFirst 5 rows of x_test (image data):\")\n",
    "# print(x_test[:2]) # Print the first 2 image arrays.\n",
    "\n",
    "# # Print the first 2 rows of y_test (labels)\n",
    "# print(\"\\nFirst 2 rows of y_test (labels):\")\n",
    "# print(y_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c4ee4738-df3b-4a32-b0b0-fc7729650c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x_train data type after conversion: float32\n",
      "  Min/Max pixel values after normalization: 0.0/1.0\n",
      "  y_train shape remains: (50000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env_ml_research/lib/python3.12/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training:\n",
    "# - Convert Image Type: Change image data type from `uint8` to `float32` for calculations.\n",
    "# - Normalize Pixels: Scale pixel values from the range [0, 255] to [0, 1]. This helps stabilize training. Alternatively, you could scale to [-1, 1] by dividing by 127.5 and subtracting 1.\n",
    "# - Labels: The labels are already integers (0-99), which is the format expected by `SparseCategoricalCrossentropy` loss. No changes needed for `y_train`, `y_test`.\n",
    "\n",
    "# Convert image data types to float32\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "\n",
    "print(f\"  x_train data type after conversion: {x_train.dtype}\") # float32\n",
    "print(f\"  Min/Max pixel values after normalization: {x_train.min():.1f}/{x_train.max():.1f}\") # 0.0/1.0\n",
    "\n",
    "# Labels y_train and y_test remain as integer arrays of shape (N, 1)\n",
    "print(f\"  y_train shape remains: {y_train.shape}\")\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\", input_shape=INPUT_SHAPE),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "\n",
    "# Apply augmentation ONLY to the training data\n",
    "# Apply augmentation within the tf.data pipeline\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train))\\\n",
    "                             .batch(BATCH_SIZE)\\\n",
    "                             .map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                             .prefetch(tf.data.AUTOTUNE) # Add prefetching\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\\\n",
    "                           .prefetch(tf.data.AUTOTUNE) # Add prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b038846-c400-425a-9daf-aafa7fbd6095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the Keras Sequential model (with Batch Norm, L2, Softmax output)...\n",
      "Model defined successfully with Batch Norm, L2, and Softmax output.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_40          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_40 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_30 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_41          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_41 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_31 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_42          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_42 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_32 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_43 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m51,300\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,196,580</span> (4.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,196,580\u001b[0m (4.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,195,108</span> (4.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,195,108\u001b[0m (4.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span> (5.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,472\u001b[0m (5.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the CNN using the `keras.Sequential` model, stacking layers linearly.\n",
    "# - input_shape: Specified in the first layer.\n",
    "# - layers.Conv2D: 2D convolution layer.\n",
    "# - layers.MaxPooling2D: Max pooling layer.\n",
    "# - layers.Flatten: Converts 3D features to 1D vector.\n",
    "# - layers.Dense: Fully connected layer.\n",
    "# - layers.Dropout: Applies dropout regularization.\n",
    "# - Final Dense Layer: Has `NUM_CLASSES` units and activation='softmax'. This makes the model output probabilities for each class.\n",
    "\n",
    "print(f\"Defining the Keras Sequential model (with Batch Norm, L2, Softmax output)...\")\n",
    "\n",
    "model = models.Sequential([\n",
    "    # Input Layer shape is defined in the first Conv2D layer\n",
    "    # Block 1\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), padding='same',\n",
    "                  kernel_regularizer=l2(L2_LAMBDA), input_shape=INPUT_SHAPE), # Removed activation, added regularizer\n",
    "    layers.BatchNormalization(), \n",
    "    layers.Activation('relu'),   \n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Output shape: (None, 16, 16, 32)\n",
    "\n",
    "    # Block 2\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='same',\n",
    "                  kernel_regularizer=l2(L2_LAMBDA)), # Removed activation, added regularizer\n",
    "    layers.BatchNormalization(), # Added Batch Norm\n",
    "    layers.Activation('relu'),   # Added separate Activation\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "    # Output shape: (None, 8, 8, 64)\n",
    "\n",
    "    # Block 3\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='same',\n",
    "                  kernel_regularizer=l2(L2_LAMBDA)), # Removed activation, added regularizer\n",
    "    layers.BatchNormalization(), # Added Batch Norm\n",
    "    layers.Activation('relu'),   # Added separate Activation\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Output shape: (None, 4, 4, 128)\n",
    "\n",
    "    # Classifier Head\n",
    "    layers.Flatten(),\n",
    "    # Output shape: (None, 4*4*128 = 2048)\n",
    "    layers.Dense(512, kernel_regularizer=l2(L2_LAMBDA)), # Removed activation, added regularizer\n",
    "    layers.BatchNormalization(), # Added Batch Norm\n",
    "    layers.Activation('relu'),   # Added separate Activation\n",
    "    layers.Dropout(0.5), # Dropout for regularization (keep after activation)\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax') # Output layer with Softmax activation!\n",
    "])\n",
    "\n",
    "print(\"Model defined successfully with Batch Norm, L2, and Softmax output.\")\n",
    "\n",
    "# Print a summary of the model's layers and parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ea7ccd4e-6530-4f7b-b1e1-6ce017e93047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Configuring the model for training.\n",
    "# - optimizer: Adam optimizer.\n",
    "# - loss: The loss function.\n",
    "# - SparseCategoricalCrossentropy: Used for multi-class classification with integer labels (0-99).\n",
    "# - metrics: [accuracy] to monitor classification accuracy.\n",
    "\n",
    "initial_learning_rate = LEARNING_RATE \n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps= (len(x_train) // BATCH_SIZE) * 5, # Decay every 5 epochs approx\n",
    "    decay_rate=0.9, # Decay factor\n",
    "    staircase=True)\n",
    "\n",
    "# Applying the schedule in the optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Define the loss function suitable for probability outputs from Softmax\n",
    "# Use SparseCategoricalCrossentropy because labels are integers (0-99)\n",
    "loss_fn = losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"Model compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0eed3c0b-c25c-4d94-9a16-2e99a3c57b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 100 epochs...\n",
      "Epoch 1/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 48ms/step - accuracy: 0.0959 - loss: 5.0181 - val_accuracy: 0.1925 - val_loss: 3.9807\n",
      "Epoch 2/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.2131 - loss: 3.7975 - val_accuracy: 0.1896 - val_loss: 3.8609\n",
      "Epoch 3/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.2566 - loss: 3.4991 - val_accuracy: 0.3047 - val_loss: 3.2974\n",
      "Epoch 4/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.2871 - loss: 3.3807 - val_accuracy: 0.1839 - val_loss: 4.1663\n",
      "Epoch 5/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.3012 - loss: 3.3179 - val_accuracy: 0.2290 - val_loss: 3.8830\n",
      "Epoch 6/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.3200 - loss: 3.2471 - val_accuracy: 0.3568 - val_loss: 3.0728\n",
      "Epoch 7/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.3324 - loss: 3.1896 - val_accuracy: 0.3156 - val_loss: 3.3312\n",
      "Epoch 8/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - accuracy: 0.3420 - loss: 3.1444 - val_accuracy: 0.3277 - val_loss: 3.2520\n",
      "Epoch 9/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.3536 - loss: 3.1062 - val_accuracy: 0.3867 - val_loss: 2.9533\n",
      "Epoch 10/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.3612 - loss: 3.0921 - val_accuracy: 0.3411 - val_loss: 3.2112\n",
      "Epoch 11/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.3682 - loss: 3.0393 - val_accuracy: 0.4178 - val_loss: 2.8426\n",
      "Epoch 12/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.3826 - loss: 3.0068 - val_accuracy: 0.3904 - val_loss: 2.9695\n",
      "Epoch 13/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.3847 - loss: 2.9620 - val_accuracy: 0.3732 - val_loss: 3.0570\n",
      "Epoch 14/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.3902 - loss: 2.9573 - val_accuracy: 0.3930 - val_loss: 3.0182\n",
      "Epoch 15/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.3922 - loss: 2.9539 - val_accuracy: 0.3516 - val_loss: 3.2168\n",
      "Epoch 16/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.4020 - loss: 2.8926 - val_accuracy: 0.4007 - val_loss: 2.9864\n",
      "Epoch 17/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4054 - loss: 2.8739 - val_accuracy: 0.4388 - val_loss: 2.7593\n",
      "Epoch 18/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4113 - loss: 2.8536 - val_accuracy: 0.4523 - val_loss: 2.7340\n",
      "Epoch 19/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4166 - loss: 2.8464 - val_accuracy: 0.4206 - val_loss: 2.8895\n",
      "Epoch 20/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4164 - loss: 2.8383 - val_accuracy: 0.3969 - val_loss: 2.9650\n",
      "Epoch 21/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4302 - loss: 2.7934 - val_accuracy: 0.4243 - val_loss: 2.8299\n",
      "Epoch 22/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4264 - loss: 2.7800 - val_accuracy: 0.4411 - val_loss: 2.7543\n",
      "Epoch 23/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4326 - loss: 2.7511 - val_accuracy: 0.4213 - val_loss: 2.8536\n",
      "Epoch 24/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4378 - loss: 2.7379 - val_accuracy: 0.4689 - val_loss: 2.6348\n",
      "Epoch 25/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 48ms/step - accuracy: 0.4312 - loss: 2.7651 - val_accuracy: 0.4255 - val_loss: 2.8567\n",
      "Epoch 26/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - accuracy: 0.4404 - loss: 2.7115 - val_accuracy: 0.4804 - val_loss: 2.5664\n",
      "Epoch 27/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 49ms/step - accuracy: 0.4511 - loss: 2.6625 - val_accuracy: 0.4471 - val_loss: 2.6832\n",
      "Epoch 28/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4477 - loss: 2.6601 - val_accuracy: 0.4904 - val_loss: 2.4973\n",
      "Epoch 29/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4480 - loss: 2.6608 - val_accuracy: 0.4166 - val_loss: 2.8445\n",
      "Epoch 30/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.4546 - loss: 2.6298 - val_accuracy: 0.4868 - val_loss: 2.5214\n",
      "Epoch 31/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4580 - loss: 2.6197 - val_accuracy: 0.4709 - val_loss: 2.5858\n",
      "Epoch 32/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4637 - loss: 2.5835 - val_accuracy: 0.4709 - val_loss: 2.5651\n",
      "Epoch 33/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4641 - loss: 2.5672 - val_accuracy: 0.4771 - val_loss: 2.5547\n",
      "Epoch 34/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 44ms/step - accuracy: 0.4649 - loss: 2.5686 - val_accuracy: 0.4390 - val_loss: 2.7682\n",
      "Epoch 35/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4632 - loss: 2.5726 - val_accuracy: 0.4442 - val_loss: 2.7390\n",
      "Epoch 36/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4683 - loss: 2.5451 - val_accuracy: 0.5001 - val_loss: 2.4398\n",
      "Epoch 37/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4689 - loss: 2.5169 - val_accuracy: 0.4869 - val_loss: 2.4671\n",
      "Epoch 38/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - accuracy: 0.4765 - loss: 2.4953 - val_accuracy: 0.5174 - val_loss: 2.3399\n",
      "Epoch 39/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4750 - loss: 2.4904 - val_accuracy: 0.4680 - val_loss: 2.5751\n",
      "Epoch 40/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4735 - loss: 2.4806 - val_accuracy: 0.5284 - val_loss: 2.2952\n",
      "Epoch 41/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4850 - loss: 2.4557 - val_accuracy: 0.4910 - val_loss: 2.4570\n",
      "Epoch 42/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4848 - loss: 2.4349 - val_accuracy: 0.4956 - val_loss: 2.4275\n",
      "Epoch 43/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.4871 - loss: 2.4338 - val_accuracy: 0.5004 - val_loss: 2.4004\n",
      "Epoch 44/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - accuracy: 0.4892 - loss: 2.4118 - val_accuracy: 0.4814 - val_loss: 2.4850\n",
      "Epoch 45/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - accuracy: 0.4885 - loss: 2.4169 - val_accuracy: 0.5101 - val_loss: 2.3638\n",
      "Epoch 46/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4959 - loss: 2.3796 - val_accuracy: 0.4927 - val_loss: 2.4384\n",
      "Epoch 47/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 47ms/step - accuracy: 0.4922 - loss: 2.3854 - val_accuracy: 0.5156 - val_loss: 2.2865\n",
      "Epoch 48/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - accuracy: 0.4970 - loss: 2.3559 - val_accuracy: 0.5135 - val_loss: 2.3230\n",
      "Epoch 49/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.4993 - loss: 2.3436 - val_accuracy: 0.5087 - val_loss: 2.3567\n",
      "Epoch 50/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.4990 - loss: 2.3356 - val_accuracy: 0.5154 - val_loss: 2.3163\n",
      "Epoch 51/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 48ms/step - accuracy: 0.5048 - loss: 2.3093 - val_accuracy: 0.5222 - val_loss: 2.2806\n",
      "Epoch 52/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5068 - loss: 2.3075 - val_accuracy: 0.5210 - val_loss: 2.2781\n",
      "Epoch 53/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 48ms/step - accuracy: 0.5053 - loss: 2.2979 - val_accuracy: 0.5207 - val_loss: 2.2733\n",
      "Epoch 54/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.5101 - loss: 2.2790 - val_accuracy: 0.5321 - val_loss: 2.2122\n",
      "Epoch 55/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.5122 - loss: 2.2742 - val_accuracy: 0.5059 - val_loss: 2.3469\n",
      "Epoch 56/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.5151 - loss: 2.2532 - val_accuracy: 0.5364 - val_loss: 2.2085\n",
      "Epoch 57/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 45ms/step - accuracy: 0.5210 - loss: 2.2380 - val_accuracy: 0.5427 - val_loss: 2.1435\n",
      "Epoch 58/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.5213 - loss: 2.2253 - val_accuracy: 0.5063 - val_loss: 2.3204\n",
      "Epoch 59/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.5161 - loss: 2.2352 - val_accuracy: 0.5345 - val_loss: 2.1842\n",
      "Epoch 60/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.5147 - loss: 2.2249 - val_accuracy: 0.5429 - val_loss: 2.1582\n",
      "Epoch 61/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5242 - loss: 2.1760 - val_accuracy: 0.5376 - val_loss: 2.1588\n",
      "Epoch 62/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5276 - loss: 2.1572 - val_accuracy: 0.5304 - val_loss: 2.2110\n",
      "Epoch 63/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5202 - loss: 2.1799 - val_accuracy: 0.5432 - val_loss: 2.1355\n",
      "Epoch 64/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 47ms/step - accuracy: 0.5352 - loss: 2.1423 - val_accuracy: 0.5244 - val_loss: 2.2302\n",
      "Epoch 65/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 46ms/step - accuracy: 0.5205 - loss: 2.1734 - val_accuracy: 0.5252 - val_loss: 2.2188\n",
      "Epoch 66/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5386 - loss: 2.1184 - val_accuracy: 0.5473 - val_loss: 2.1260\n",
      "Epoch 67/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5388 - loss: 2.1041 - val_accuracy: 0.5450 - val_loss: 2.1019\n",
      "Epoch 68/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5368 - loss: 2.1157 - val_accuracy: 0.5399 - val_loss: 2.1404\n",
      "Epoch 69/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5404 - loss: 2.0822 - val_accuracy: 0.5383 - val_loss: 2.1700\n",
      "Epoch 70/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5358 - loss: 2.1087 - val_accuracy: 0.5457 - val_loss: 2.1142\n",
      "Epoch 71/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5420 - loss: 2.0749 - val_accuracy: 0.5352 - val_loss: 2.1446\n",
      "Epoch 72/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.5447 - loss: 2.0572 - val_accuracy: 0.5650 - val_loss: 2.0162\n",
      "Epoch 73/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5466 - loss: 2.0521 - val_accuracy: 0.5447 - val_loss: 2.1101\n",
      "Epoch 74/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 43ms/step - accuracy: 0.5425 - loss: 2.0459 - val_accuracy: 0.5382 - val_loss: 2.1541\n",
      "Epoch 75/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5468 - loss: 2.0496 - val_accuracy: 0.5553 - val_loss: 2.0246\n",
      "Epoch 76/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 43ms/step - accuracy: 0.5515 - loss: 2.0237 - val_accuracy: 0.5556 - val_loss: 2.0407\n",
      "Epoch 77/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5531 - loss: 2.0024 - val_accuracy: 0.5682 - val_loss: 1.9782\n",
      "Epoch 78/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5568 - loss: 1.9926 - val_accuracy: 0.5572 - val_loss: 2.0767\n",
      "Epoch 79/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5572 - loss: 1.9970 - val_accuracy: 0.5589 - val_loss: 2.0047\n",
      "Epoch 80/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5560 - loss: 1.9770 - val_accuracy: 0.5483 - val_loss: 2.0449\n",
      "Epoch 81/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5593 - loss: 1.9710 - val_accuracy: 0.5474 - val_loss: 2.0700\n",
      "Epoch 82/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5595 - loss: 1.9532 - val_accuracy: 0.5602 - val_loss: 2.0097\n",
      "Epoch 83/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5598 - loss: 1.9559 - val_accuracy: 0.5277 - val_loss: 2.1344\n",
      "Epoch 84/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5597 - loss: 1.9549 - val_accuracy: 0.5579 - val_loss: 2.0064\n",
      "Epoch 85/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5573 - loss: 1.9603 - val_accuracy: 0.5764 - val_loss: 1.9356\n",
      "Epoch 86/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5626 - loss: 1.9396 - val_accuracy: 0.5671 - val_loss: 1.9649\n",
      "Epoch 87/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5681 - loss: 1.9044 - val_accuracy: 0.5627 - val_loss: 1.9761\n",
      "Epoch 88/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5626 - loss: 1.9032 - val_accuracy: 0.5705 - val_loss: 1.9269\n",
      "Epoch 89/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.5631 - loss: 1.9164 - val_accuracy: 0.5701 - val_loss: 1.9572\n",
      "Epoch 90/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5709 - loss: 1.8902 - val_accuracy: 0.5721 - val_loss: 1.9154\n",
      "Epoch 91/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5733 - loss: 1.8755 - val_accuracy: 0.5624 - val_loss: 1.9786\n",
      "Epoch 92/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.5671 - loss: 1.8856 - val_accuracy: 0.5808 - val_loss: 1.8797\n",
      "Epoch 93/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5692 - loss: 1.8749 - val_accuracy: 0.5583 - val_loss: 1.9923\n",
      "Epoch 94/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 39ms/step - accuracy: 0.5688 - loss: 1.8859 - val_accuracy: 0.5684 - val_loss: 1.9258\n",
      "Epoch 95/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5688 - loss: 1.8599 - val_accuracy: 0.5776 - val_loss: 1.8774\n",
      "Epoch 96/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5842 - loss: 1.8242 - val_accuracy: 0.5815 - val_loss: 1.8689\n",
      "Epoch 97/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5738 - loss: 1.8461 - val_accuracy: 0.5739 - val_loss: 1.9044\n",
      "Epoch 98/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5847 - loss: 1.8142 - val_accuracy: 0.5725 - val_loss: 1.8992\n",
      "Epoch 99/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5795 - loss: 1.8227 - val_accuracy: 0.5726 - val_loss: 1.8908\n",
      "Epoch 100/100\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.5840 - loss: 1.8076 - val_accuracy: 0.5851 - val_loss: 1.8216\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Training the model using the `model.fit()` method.\n",
    "\n",
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=test_dataset, # Evaluate on test set after each epoch\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "deb96a12-07e0-4b6a-ba73-c3846ccf7e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the model on the test dataset...\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5784 - loss: 1.8437\n",
      "\n",
      "Test Loss: 1.8216\n",
      "Test Accuracy: 58.51%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the trained model's performance on the test dataset using `model.evaluate()`.\n",
    "# - Pass the test data (`test_dataset`).\n",
    "# - It returns the final loss and metric values (e.g., accuracy) calculated on the test set.\n",
    "\n",
    "print(\"\\nEvaluating the model on the test dataset...\")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(\n",
    "    test_dataset, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010a9ea-1ce3-4c24-a912-5367616fd98b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_ml_research]",
   "language": "python",
   "name": "conda-env-env_ml_research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
